
Layoff Data Cleaning for EDA


##Overview

This project focuses on cleaning a messy global layoff dataset from Kaggle to prepare it for Exploratory Data Analysis (EDA).
The goal was to fix duplicates, standardize data formats, handle missing values, and tidy up columns â€” 
so the data is ready for real analysis.


##About This Project

This cleaning project is part of the 'Alex The Analyst Bootcamp',
 where I sharpened my skills on real-world datasets.
It helped me get hands-on experience with SQL data cleaning techniques



##Dataset

Source: Layoffs 2022 Dataset on Kaggle
The dataset covers global layoffs, including info on companies, locations, industries, dates, and layoff numbers.


##What I Did

1. Created a safe staging table to work on without touching original data

2. Found and removed duplicate records using SQL window functions (ROW_NUMBER) and CTEs

3. Cleaned and standardized text fields like company names, industries, and locations

4. Converted date fields from text to proper DATE type for better analysis

5. Handled NULLs and blanks, replacing blanks with NULLs and filling missing values where possible

6. Dropped unnecessary or unreliable columns and rows to improve data quality



##Tools & Skills Used

-SQL (MySQL) for data cleaning and manipulation
-Techniques: deduplication, text standardization, null handling
-GitHub for documenting and sharing code


##What I Learned

-How to clean messy real-world data in SQL
-Writing clear, commented SQL scripts to explain every step

## ðŸ“¬ Contact Me
- **LinkedIn**: Ruqaya Mohammed
- URL: http://www.linkedin.com/in/ruqaya-sql
-Email: mm.rr12345.com@gmail.com 
